from transformers import (
    pipeline,
    GPT2ForSequenceClassification,
    GPT2Tokenizer,
    TextClassificationPipeline,
    GPT2LMHeadModel,
    TextGenerationPipeline,
)
import random
from pathlib import Path, PurePath
import os
import distutils


class Bot:
    name = "Funnybot"

    _transformers_dir = Path(__file__).parent / "transformers"

    evaluator_dir = _transformers_dir / "joke-evaluator"
    generator_dir = _transformers_dir / "joke-generator"

    def __init__(self):
        self.evaluator_pipeline = self._load_evaluator_pipeline()
        self.generator_pipelines = self._load_generator_pipelines()

    def tell_joke(self) -> str:
        """
        Tells a joke according to the user's choice.

        Different types of jokes are available, e.g., "Dad Jokes", "Question Answer Jokes", which one with its own pipeline.

        This method iterates over the available selection of pipelines and prompts the user to ask if a particular one is of
        ones liking.
        """
        joke = "Sorry, I don't have any other jokes. That's just Awkward."
        affirmative_choices = ["yes", "y"]
        catchphrase = "Awkwaaard!"

        for joke_type, pipeline in self.generator_pipelines.items():
            choice = (
                input(f"Do you like {joke_type}? yes/no: ") or affirmative_choices[0]
            )
            if choice.lower() in affirmative_choices:
                joke = pipeline("")[0]["generated_text"]
                joke = f"{joke} {catchphrase}"
                break

        return joke

    def rate_joke(self, joke):
        rating = self.evaluator_pipeline(joke)[0]["label"]
        return rating

    @classmethod
    def _load_generator_pipelines(cls) -> dict[str, TextGenerationPipeline]:
        """
        Loads all available joke generator pipelines. The models and the tokenizer used by the pipelines are organized by
        directory, each directory containing models for different types of jokes, e.g. "Dad Jokes", "Question Answer Jokes".

        All models and the tokenizer are generated by this Jupyter notebook:
            `<project root>/bots/funnybot/transformers/notebooks/Joke Generator.ipynb`.
        """
        model_repository_prefix = "marciogualtieri/funnybot-joke-generator-model"
        tokenizer_repository = "marciogualtieri/funnybot-joke-generator-tokenizer"
        models_dir = cls.generator_dir / "models"

        models = {}
        for model_dir in Path.iterdir(models_dir):
            if model_dir.is_dir():
                model_name = PurePath(model_dir).name
                model_repository_suffix = model_name.lower().replace(" ", "-")
                models[model_name] = (
                    model_dir
                    if cls._is_local()
                    else f"{model_repository_prefix}-{model_repository_suffix}"
                )

        tokenizer = GPT2Tokenizer.from_pretrained(
            cls.generator_dir / "tokenizer" if cls._is_local() else tokenizer_repository
        )

        pipelines = {}
        for model_name, model_location in models.items():
            model = GPT2LMHeadModel.from_pretrained(model_location)
            pipelines[model_name] = TextGenerationPipeline(
                model=model, tokenizer=tokenizer
            )

        return pipelines

    @classmethod
    def _load_evaluator_pipeline(cls) -> TextClassificationPipeline:
        """
        Loads the joke evaluator pipeline.

        The model and tokenizer used by the pipeline are generated by this Jupyter notebook:
            `<project root>/bots/funnybot/transformers/notebooks/Joke Evaluator.ipynb`.
        """

        evaluator_model = GPT2ForSequenceClassification.from_pretrained(
            cls.evaluator_dir / "model"
            if cls._is_local()
            else "marciogualtieri/funnybot-joke-evaluator-model"
        )
        evaluator_tokenizer = GPT2Tokenizer.from_pretrained(
            cls.evaluator_dir / "tokenizer"
            if cls._is_local()
            else "marciogualtieri/funnybot-joke-evaluator-tokenizer"
        )
        return TextClassificationPipeline(
            model=evaluator_model, tokenizer=evaluator_tokenizer
        )

    @classmethod
    def _is_local(cls) -> bool:
        try:
            return bool(distutils.util.strtobool(os.environ.get("IS_LOCAL", "false")))
        except ValueError:
            return False
